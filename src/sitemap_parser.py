import requests
import logging
from bs4 import BeautifulSoup
from datetime import datetime
import os
import json

logger = logging.getLogger(__name__)

class SitemapParser:
    def __init__(self, sitemap_url, cache_file='last_articles.json'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ sitemap
        
        :param sitemap_url: URL sitemap.xml
        :param cache_file: –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞–Ω–µ–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        """
        self.sitemap_url = sitemap_url
        self.cache_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), cache_file)
        self.last_articles = self._load_last_articles()
    
    def _load_last_articles(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞–Ω–µ–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ –∫–µ—à–∞"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞ —Å—Ç–∞—Ç–µ–π: {e}")
            return {}
    
    def _save_last_articles(self, articles):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –∫–µ—à"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞ —Å—Ç–∞—Ç–µ–π: {e}")
    
    def parse_sitemap(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ sitemap.xml –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π"""
        try:
            response = requests.get(self.sitemap_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'xml')
            urls = soup.find_all('url')
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –≤ sitemap")
            
            articles = {}
            for url in urls:
                loc = url.find('loc')
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ lastmod, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç, —Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º changefreq –∏ priority
                lastmod = url.find('lastmod')
                changefreq = url.find('changefreq')
                priority = url.find('priority')
                
                if loc:
                    url_text = loc.text
                    
                    # –ï—Å–ª–∏ –µ—Å—Ç—å lastmod, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                    if lastmod:
                        modified_text = lastmod.text
                    # –ï—Å–ª–∏ –Ω–µ—Ç lastmod, –Ω–æ –µ—Å—Ç—å changefreq –∏–ª–∏ priority, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –∫–∞–∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–µ—Ä—Å–∏–∏
                    elif changefreq or priority:
                        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
                        cf_text = changefreq.text if changefreq else "daily"
                        pr_text = priority.text if priority else "0.5"
                        modified_text = f"{cf_text}_{pr_text}_{datetime.now().strftime('%Y-%m-%d')}"
                    else:
                        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ç–µ–≥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
                        modified_text = datetime.now().strftime("%Y-%m-%d")
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –±—ã–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –¥–∞–π–¥–∂–µ—Å—Ç–µ
                    if url_text in self.last_articles and self.last_articles[url_text] == modified_text:
                        continue
                    
                    articles[url_text] = modified_text
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤—ã—Ö –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π
            new_articles = []
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Ä–µ—Å—É—Ä—Å—ã
            max_articles_to_process = 100
            for i, url in enumerate(list(articles.keys())[:max_articles_to_process]):
                try:
                    title = self._get_article_title(url)
                    if title:
                        new_articles.append({
                            'url': url,
                            'title': title,
                            'lastmod': articles[url]
                        })
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –¥–∞—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
            def get_priority(article):
                try:
                    if '_' in article['lastmod']:
                        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç cf_pr_date
                        return float(article['lastmod'].split('_')[1])
                    return 0.5
                except:
                    return 0.5
                    
            new_articles.sort(key=get_priority, reverse=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
            self._save_last_articles(articles)
            
            return new_articles
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ sitemap: {e}")
            return []
    
    def _get_article_title(self, url):
        """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ –ø–æ URL"""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.find('title')
            
            if title:
                return title.text.strip()
            
            # –ï—Å–ª–∏ title –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ h1
            h1 = soup.find('h1')
            if h1:
                return h1.text.strip()
            
            return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è {url}: {e}")
            return "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    
    def format_digest(self, articles, max_articles=10):
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π
        
        :param articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
        :param max_articles: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å—Ç–∞—Ç–µ–π –≤ –¥–∞–π–¥–∂–µ—Å—Ç–µ
        :return: –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–∞–π–¥–∂–µ—Å—Ç–∞
        """
        if not articles:
            return "–ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π"
        
        articles = articles[:max_articles]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
        
        now = datetime.now().strftime("%d.%m.%Y %H:%M")
        digest = f"üì∞ *–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –æ—Ç {now}*\n\n"
        
        for i, article in enumerate(articles, 1):
            digest += f"*{i}. {article['title']}*\n{article['url']}\n\n"
        
        digest += "ü§ñ –ë–æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞–∂–¥—ã–π —á–∞—Å"
        
        return digest 